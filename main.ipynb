{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea054c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from torch import einsum\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc92a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weblogo_opts = '-X NO -Y NO --errorbars NO --fineprint \"\"'\n",
    "weblogo_opts = '-X NO --fineprint \"\"'\n",
    "weblogo_opts += ' -C \"#CB2026\" A A'\n",
    "weblogo_opts += ' -C \"#34459C\" C C'\n",
    "weblogo_opts += ' -C \"#FBB116\" G G'\n",
    "# embed\n",
    "weblogo_opts += ' -C \"#0C8040\" T T'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e64639",
   "metadata": {},
   "source": [
    "# DATA Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ff4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chrom_train_test(peaks,Chr_dict):\n",
    "    peak_chromosomes = {}\n",
    "    n_peaks = 0\n",
    "    chrom_train = []\n",
    "    chrom_test = []\n",
    "    for line in peaks:\n",
    "        split_line = 'chromosome='+line.split('\\t')[0]\n",
    "        peak_chromosomes[split_line] = peak_chromosomes.get(split_line,0) + 1\n",
    "        n_peaks+=1\n",
    "    n_peaks-=peak_chromosomes.pop('chromosome=pombeIII')\n",
    "    size_test = int(n_peaks/10)\n",
    "    npeaks_test = 0\n",
    "    while npeaks_test<size_test:\n",
    "        chrom = random.choice(list(Chr_dict.keys()))\n",
    "        npeaks_test += peak_chromosomes[chrom]\n",
    "        chrom_test.append(chrom)\n",
    "        if npeaks_test>int(n_peaks*1.3/10):\n",
    "            npeaks_test = 0\n",
    "            chrom_test = []\n",
    "    for chrom in Chr_dict.keys():\n",
    "        if chrom not in chrom_test:\n",
    "            chrom_train.append(chrom)\n",
    "    return chrom_train,chrom_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4831bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_data(data_file):\n",
    "    data=open(data_file).read()\n",
    "    chromosomes_data = data.split('>')[1:]\n",
    "    return (chromosomes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a84bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_bed_file(chromosomes_data,peak_file,seq_length,Chr_dict,cross_chrom):\n",
    "    peaks = open(peak_file).readlines()\n",
    "    peak_sequences = []\n",
    "    if cross_chrom:\n",
    "        peak_sequences_train,peak_sequences_test = [],[]\n",
    "        chrom_train , chrom_test = generate_chrom_train_test(peaks,Chr_dict)\n",
    "    for peak in peaks:\n",
    "        peak_split = peak.split('\\t')\n",
    "        Chr = 'chromosome='+str(peak_split[0])\n",
    "        if Chr in Chr_dict:\n",
    "            chrom_seq = Chr_dict[Chr]\n",
    "            n = len(chrom_seq)\n",
    "            start_idx = max(int(peak_split[1])-seq_length//2,0)\n",
    "            end_idx = min(int(len(chrom_seq)), start_idx+seq_length)\n",
    "            if end_idx == len(chrom_seq):\n",
    "                start_idx = end_idx - seq_length\n",
    "            header = Chr+':{0}-{1}'.format(start_idx,end_idx)\n",
    "            if not cross_chrom:\n",
    "                peak_sequences.append([header,chrom_seq[start_idx:end_idx]])\n",
    "            else:\n",
    "                if Chr in chrom_train:\n",
    "                    peak_sequences_train.append([header,chrom_seq[start_idx:end_idx]])\n",
    "                else:\n",
    "                    peak_sequences_test.append([header,chrom_seq[start_idx:end_idx]])\n",
    "    if not cross_chrom:\n",
    "        size=int(len(peak_sequences)/10)\n",
    "        peak_sequences_train = peak_sequences[:9*size]\n",
    "        peak_sequences_test = peak_sequences[9*size:]\n",
    "    return (peak_sequences_train,peak_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dinucshuffle(sequence):\n",
    "    b=[sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
    "    random.shuffle(b)\n",
    "    d=''.join([str(x) for x in b])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584b788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqtopad(sequence, motif_len):\n",
    "    rows=len(sequence)+2*motif_len-2\n",
    "    S=np.empty([rows,4])\n",
    "    base=['A', 'C', 'G', 'T']\n",
    "    for i in range(rows):\n",
    "        for j in range(4):\n",
    "            if (i-motif_len+1<len(sequence) and sequence[i-motif_len+1]=='N' \n",
    "                or i<motif_len-1 or i>len(sequence)+motif_len-2):\n",
    "                S[i,j]=np.float32(0.25)\n",
    "            elif sequence[i-motif_len+1]==base[j]:\n",
    "                S[i,j]=np.float32(1)\n",
    "            else:\n",
    "                S[i,j]=np.float32(0)\n",
    "    return np.transpose(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0f38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_data(peak_sequences,motif_length,label,include_dinuc):\n",
    "    alldata = []\n",
    "    for header,seq in peak_sequences:\n",
    "        alldata.append([header,seq,seqtopad(seq,motif_length),[int(label)]])#\n",
    "        if include_dinuc:\n",
    "            shuff_seq = dinucshuffle(seq)\n",
    "            alldata.append([header,shuff_seq,seqtopad(shuff_seq,motif_length),[0]])#\n",
    "    return (alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d582c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_file,peak_file,motif_length=24,seq_length=150,cross_chrom=False,include_dinuc=True,Chr_dict=None):\n",
    "    chromosomes_data = genome_data(data_file)\n",
    "    if Chr_dict==None:\n",
    "        Chr_dict = {}\n",
    "        for chrom_data in chromosomes_data:\n",
    "            ref = chrom_data.split('\\n')[0].split(' ')[-1][1:-1]\n",
    "            Chr_dict[ref]=''.join(chrom_data.split('\\n')[1:])\n",
    "        Chr_dict['chromosome=Mito'] = Chr_dict.pop('top=circular')\n",
    "    if type(peak_file) == str:\n",
    "        peak_sequences_train,peak_sequences_test = Read_bed_file(chromosomes_data,peak_file,seq_length,Chr_dict,cross_chrom)\n",
    "        train_data = generate_onehot_data(peak_sequences_train,motif_length,1,include_dinuc)\n",
    "        test_data = generate_onehot_data(peak_sequences_test,motif_length,1,include_dinuc)\n",
    "        random.shuffle(train_data)\n",
    "        size=int(len(train_data)/10)\n",
    "        calib_data=train_data[:9*size]\n",
    "        valid_data=train_data[9*size:]\n",
    "    elif type(peak_file) == list: #Using the data in each file as samples associated with 1 label.\n",
    "        peak_sequences_train,peak_sequences_test = [],[]\n",
    "        train_data,test_data = [],[]\n",
    "        for i in range (len(peak_file)):\n",
    "            peak_sequences_train_temp,peak_sequences_test_temp = Read_bed_file(chromosomes_data,peak_file[i],seq_length,Chr_dict,cross_chrom)\n",
    "            peak_sequences_train.extend(peak_sequences_train_temp)\n",
    "            peak_sequences_test.extend(peak_sequences_test_temp)\n",
    "            label = 1 if include_dinuc else i \n",
    "            train_data.extend(generate_onehot_data(peak_sequences_train_temp,motif_length,label,include_dinuc))\n",
    "            test_data.extend(generate_onehot_data(peak_sequences_test_temp,motif_length,label,include_dinuc))\n",
    "        random.shuffle(train_data)\n",
    "        random.shuffle(test_data)\n",
    "        size=int(len(train_data)/10)\n",
    "        calib_data=train_data[:9*size]\n",
    "        valid_data=train_data[9*size:]\n",
    "    return calib_data,valid_data,train_data,test_data,peak_sequences_train,peak_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c6057d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, xy=None):\n",
    "        self.header=[el[0] for el in xy]\n",
    "        self.seq =[el[1] for el in xy ]\n",
    "        self.x_data=np.asarray([el[2] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[3] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.length=len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.header[index],self.seq[index],self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef79b14",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a7e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.to_attn_logits = nn.Parameter(torch.eye(dim)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_logits = einsum('b n d, d e -> b n e', x, self.to_attn_logits) \n",
    "        attn = attn_logits.softmax(dim = -2) \n",
    "        return (x * attn).sum(dim = -2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e31aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_motif , motif_len , num_conv_layers , dropprob):\n",
    "        super(Network, self).__init__()\n",
    "        self.num_motif = num_motif\n",
    "        self.conv = [nn.Conv1d(4, num_motif, kernel_size=motif_len),nn.ReLU(inplace=True)]\n",
    "        in_channels = num_motif\n",
    "        for i in range (num_conv_layers-1):\n",
    "            motif_len = motif_len//2\n",
    "            self.conv.append(nn.MaxPool1d(kernel_size=3))\n",
    "            self.conv.append(nn.Conv1d(in_channels, int(1.5*in_channels), kernel_size=motif_len))\n",
    "            self.conv.append(nn.ReLU(inplace=True))\n",
    "            in_channels = int(1.5*in_channels)\n",
    "        self.conv_layer = nn.Sequential(*self.conv)\n",
    "        self.project = AttentionPool(in_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_channels , in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False),\n",
    "            nn.Linear(in_channels, 1),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x= x.permute(0, 2, 1)\n",
    "        x = self.project(x)\n",
    "        predict = self.classifier(x)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fb2ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_output(nn.Module):\n",
    "    def __init__(self,filter_weights,filter_bias,device):\n",
    "        super(conv_output, self).__init__()\n",
    "        if type(filter_weights) is np.ndarray:\n",
    "            self.filter_weights =  torch.from_numpy(filter_weights.astype(np.float32)).to(device)\n",
    "        else :\n",
    "            self.filter_weights = filter_weights.to(device)\n",
    "        if type(filter_bias) is np.ndarray:\n",
    "            self.filter_bias =  torch.from_numpy(filter_bias.astype(np.float32)).to(device)\n",
    "        else :\n",
    "            self.filter_bias = filter_bias.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.conv1d(x, self.filter_weights, bias=self.filter_bias, stride=1, padding=0)\n",
    "        out=x.clamp(min=0)\n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0649a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### printing parameters ------------------\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table.get_string())\n",
    "    print(f\"\\n Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d6ce6",
   "metadata": {},
   "source": [
    "# Calib - Train - Test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79573514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(model,train_loader,valid_loader, l_rate=0.01 , maxepochs=100,epochs_for_early_stop=0,save_model=False):\n",
    "    if not new:\n",
    "        try:\n",
    "            best_model = torch.load(model_dir+'/best_model.pkl')\n",
    "            return(best_model,maxepochs)\n",
    "        except:\n",
    "            print('Pretrained model not found. Training the model')\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "    nepochs=0\n",
    "    valid_losses =[]\n",
    "    train_losses = []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=l_rate,weight_decay=1e-05)\n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    while nepochs<maxepochs:\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for i, (header, seq, data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)#\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "        if verbose:\n",
    "            print('Model trained for {0} epochs out of {2}. Training loss is {1}'.format(nepochs+1,loss.item(),maxepochs))\n",
    "        train_losses.append(train_loss/(i+1))\n",
    "        with torch.no_grad():\n",
    "            model.eval() \n",
    "            valid_loss=0\n",
    "            for i, (header, seq, data, target) in enumerate(valid_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target)#\n",
    "                valid_loss+=loss.item()\n",
    "            valid_losses.append(valid_loss/(i+1))\n",
    "        counter+=1\n",
    "        nepochs +=1\n",
    "        if epochs_for_early_stop>0:\n",
    "            if valid_losses[-1]<best_loss:\n",
    "                if verbose:\n",
    "                    print('Validation loss decreased from {0} to {1}'.format(best_loss,valid_losses[-1]))\n",
    "                best_loss = valid_losses[-1]\n",
    "                best_model = model\n",
    "                counter = 0\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Counter for early stopping: {0} out of {1}'.format(counter,epochs_for_early_stop))\n",
    "                if counter == epochs_for_early_stop:\n",
    "                    print('early stopping at epoch ', nepochs-counter)\n",
    "                    if save_model:\n",
    "                        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "                        count_parameters(best_model)\n",
    "                    return (best_model,nepochs-counter)\n",
    "    print('no early stopping')\n",
    "    if save_model:\n",
    "        torch.save(model,model_dir+'/best_model.pkl')\n",
    "        count_parameters(model)\n",
    "    return (model,nepochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88e481f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_model(model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        labels_list = []\n",
    "        for i, (header, seq, data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "            pred_list.append(pred)\n",
    "            labels_list.append(labels)\n",
    "        labels = np.concatenate(labels_list)\n",
    "        predictions = np.concatenate(pred_list)\n",
    "    auc = metrics.roc_auc_score(labels, predictions)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n",
    "    prc = (metrics.auc(recall, precision))\n",
    "    file = open('%s/results.txt'%results_dir, 'w')\n",
    "    header_cols = ('N_test_samples','AUROC', 'AURPC')\n",
    "    print('%5s  %10s  %10s' % header_cols, file=file)\n",
    "    row_cols = (len(predictions),auc,prc)\n",
    "    print('%5i  %19.4f  %10.4f'%row_cols,file=file)\n",
    "    if verbose:\n",
    "        print('AUROC on test data ', auc)\n",
    "        print('AUPRC on test data ', prc)\n",
    "    return (auc,prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e24718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibrate_model(calib_loader,valid_loader, num_motif_list, num_conv_layers_list , dropprob_list,learning_rate_list, \n",
    "                    max_num_models=40, maxepochs=100,epochs_for_early_stop=0 , motif_len=24 ):\n",
    "    if not new:\n",
    "        try:\n",
    "            best_hyperparameters=torch.load(model_dir+'/best_hyperpamarameters.pth')\n",
    "            results = pd.read_csv(model_dir+'/calibration_df.csv')\n",
    "            return(best_hyperparameters,results)\n",
    "        except:\n",
    "            print('Calibration results not found. Calibrating the model')\n",
    "    results=pd.DataFrame(columns=['num_conv_layers','num_motif','Dropout','Learning Rate','epochs','AUROC','AUPRC'])\n",
    "    best_AUC = 0\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    for number in range(max_num_models):\n",
    "        print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        num_motif = random.choice(num_motif_list)\n",
    "        num_conv_layers = random.choice(num_conv_layers_list)\n",
    "        dropprob = random.choice(dropprob_list)\n",
    "        l_rate = random.choice(learning_rate_list)\n",
    "        while ((results['num_conv_layers']==num_conv_layers) & (results['num_motif']==num_motif) \n",
    "               & (results['Dropout']==dropprob) & (results['Learning Rate']==l_rate)).any(): \n",
    "            #if hyperparameters exist in the results dataframe then randomly choose other parameters\n",
    "            num_motif = random.choice(num_motif_list)\n",
    "            num_conv_layers = random.choice(num_conv_layers_list)\n",
    "            dropprob = random.choice(dropprob_list)\n",
    "            l_rate = random.choice(learning_rate_list)\n",
    "        model = Network(num_motif , motif_len , num_conv_layers , dropprob).to(device)#num_conv_layers,dropprob\n",
    "        best_model,epochs = Train_model(model,calib_loader,valid_loader,l_rate ,maxepochs,epochs_for_early_stop)\n",
    "        auc,prc = Test_model(best_model,valid_loader)\n",
    "        results=pd.concat([results,pd.DataFrame({'num_conv_layers':num_conv_layers,'num_motif':num_motif,'Dropout':dropprob,\n",
    "                                                 'Learning Rate':l_rate,'epochs':epochs,'AUROC':[auc],'AUPRC':[prc]})])\n",
    "        if auc > best_AUC :\n",
    "            best_AUC = auc\n",
    "            best_epochs = epochs\n",
    "            best_num_motif = num_motif\n",
    "            best_num_conv_layers = num_conv_layers\n",
    "            best_dropprob = dropprob\n",
    "            best_l_rate = l_rate\n",
    "    best_hyperparameters = {'best_epochs': best_epochs,'best_num_motif':best_num_motif,\n",
    "                            'best_num_conv_layers':best_num_conv_layers,'best_dropprob':best_dropprob,'best_l_rate':best_l_rate}\n",
    "    torch.save(best_hyperparameters, model_dir+'/best_hyperpamarameters.pth')\n",
    "    results.sort_values(by='AUROC',ascending=False,inplace=True)\n",
    "    results.to_csv(model_dir+'/calibration_df.csv',index=False)\n",
    "    return best_hyperparameters,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c32a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_filter_outputs(model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        pred_list = []\n",
    "        for i, (header, seq, data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred=output.cpu().detach().numpy()\n",
    "            pred_list.append(pred)\n",
    "        predictions = np.concatenate(pred_list)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532222b",
   "metadata": {},
   "source": [
    "# Motif extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ebd21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_content(pwm, transpose=False, bg_gc=0.415):\n",
    "    ''' Compute PWM information content.\n",
    "    In the original analysis, I used a bg_gc=0.5. For any\n",
    "    future analysis, I ought to switch to the true hg19\n",
    "    value of 0.415.\n",
    "    '''\n",
    "    pseudoc = 1e-9\n",
    "    if transpose:\n",
    "        pwm = np.transpose(pwm)\n",
    "\n",
    "    bg_pwm = [1-bg_gc, bg_gc, bg_gc, 1-bg_gc]\n",
    "\n",
    "    ic = 0\n",
    "    for i in range(pwm.shape[0]):\n",
    "        for j in range(4):\n",
    "            # ic += 0.5 + pwm[i][j]*np.log2(pseudoc+pwm[i][j])\n",
    "            ic += -bg_pwm[j]*np.log2(bg_pwm[j]) + pwm[i][j]*np.log2(pseudoc+pwm[i][j])\n",
    "    return ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76c17aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_intro(meme_file, seqs):\n",
    "    ''' Open MEME motif format file and print intro\n",
    "    Attrs:\n",
    "        meme_file (str) : filename\n",
    "        seqs [str] : list of strings for obtaining background freqs\n",
    "    Returns:\n",
    "        mem_out : open MEME file\n",
    "    '''\n",
    "    nts = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "\n",
    "    # count\n",
    "    nt_counts = [1]*4\n",
    "    for i in range(len(seqs)):\n",
    "        for nt in seqs[i][1]:\n",
    "            try:\n",
    "                nt_counts[nts[nt]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # normalize\n",
    "    nt_sum = float(sum(nt_counts))\n",
    "    nt_freqs = [nt_counts[i]/nt_sum for i in range(4)]\n",
    "\n",
    "    # open file for writing\n",
    "    meme_out = open(meme_file, 'w')\n",
    "\n",
    "    # print intro material\n",
    "    print( 'MEME version 4', file=meme_out)\n",
    "    print( '', file=meme_out)\n",
    "    #embd\n",
    "\n",
    "    print( 'ALPHABET= ACGT', file=meme_out)        \n",
    "    \n",
    "    print( '', file=meme_out)\n",
    "    print( 'Background letter frequencies:', file=meme_out)\n",
    "    #embd\n",
    "    print( 'A %.4f C %.4f G %.4f T %.4f' % tuple(nt_freqs), file=meme_out)\n",
    "    print( '', file=meme_out)\n",
    "    return meme_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d58a1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter_pwm(filter_fasta):\n",
    "    ''' Make a PWM for this filter from its top hits '''\n",
    "    nts = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "    #embd\n",
    "    pwm_counts = []\n",
    "    nsites = 4 # pseudocounts\n",
    "    for line in open(filter_fasta):\n",
    "        if line[0] != '>':\n",
    "            seq = line.rstrip()\n",
    "            nsites += 1\n",
    "            if len(pwm_counts) == 0:\n",
    "                # initialize with the length\n",
    "                for i in range(len(seq)):\n",
    "                    pwm_counts.append(np.array([1.0]*4))\n",
    "\n",
    "            # count\n",
    "            for i in range(len(seq)):\n",
    "                try:\n",
    "                    pwm_counts[i][nts[seq[i]]] += 1\n",
    "                except KeyError:\n",
    "                    pwm_counts[i] += np.array([0.25]*4)\n",
    "\n",
    "    # normalize\n",
    "    pwm_freqs = []\n",
    "    for i in range(len(pwm_counts)):\n",
    "        pwm_freqs.append([pwm_counts[i][j]/float(nsites) for j in range(4)])\n",
    "\n",
    "    return np.array(pwm_freqs), nsites-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58673d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_density(f_scores, out_pdf):\n",
    "    sns.set(font_scale=1.3)\n",
    "    plt.figure()\n",
    "    sns.distplot(f_scores, kde=False)\n",
    "    plt.xlabel('ReLU output')\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()\n",
    "\n",
    "    return f_scores.mean(), f_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "948f58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_motif(param_matrix):\n",
    "    nts = 'ACGT'\n",
    "    motif_list = []\n",
    "    for v in range(param_matrix.shape[1]):\n",
    "        max_n = 0\n",
    "        for n in range(1,4):\n",
    "            if param_matrix[n,v] > param_matrix[max_n,v]:\n",
    "                max_n = n\n",
    "\n",
    "        if param_matrix[max_n,v] > 0:\n",
    "            motif_list.append(nts[max_n])\n",
    "        else:\n",
    "            motif_list.append('N')\n",
    "\n",
    "    return ''.join(motif_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc48f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_heat(param_matrix, out_pdf):\n",
    "    param_range = abs(param_matrix).max()\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(param_matrix.shape[1], 4))\n",
    "    sns.heatmap(param_matrix, cmap='PRGn', linewidths=0.2, vmin=-param_range, vmax=param_range)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(range(1,param_matrix.shape[1]+1))\n",
    "    ax.set_yticklabels('ACGT', rotation='horizontal') # , size=10)\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cebfb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_logo(filter_outs, filter_size, seqs, out_prefix, filter_num, raw_t=0, maxpct_t=None):\n",
    "    if maxpct_t:\n",
    "        all_outs = np.ravel(filter_outs)\n",
    "        all_outs_mean = all_outs.mean()\n",
    "        all_outs_norm = all_outs - all_outs_mean\n",
    "        raw_t = maxpct_t * all_outs_norm.max() + all_outs_mean\n",
    "    # print fasta file of positive outputs\n",
    "    filter_fasta_out = open('%s.fa' % out_prefix, 'w')\n",
    "    filter_count = 0\n",
    "    for i in range(filter_outs.shape[0]):\n",
    "        for j in range(filter_outs.shape[1]):\n",
    "            if filter_outs[i,j] > raw_t:\n",
    "                kmer = seqs[i][1][j:j+filter_size]\n",
    "                chrom = motif_sequences[i][0].split(':')[0]\n",
    "                pos = int(motif_sequences[i][0].split(':')[1].split('-')[0])+j\n",
    "                try:\n",
    "                    filter_hits[filter_num][chrom].append(pos)\n",
    "                except:\n",
    "                    filter_hits[filter_num][chrom] = [pos]\n",
    "                #kmer = kmer.replace('T','U')\n",
    "                incl_kmer = len(kmer) - kmer.count('N')\n",
    "                if incl_kmer <filter_size:\n",
    "                    continue\n",
    "                print('>%d_%d' % (i,j), file=filter_fasta_out)\n",
    "                print(kmer, file=filter_fasta_out)\n",
    "                filter_count += 1\n",
    "    filter_fasta_out.close()\n",
    "    # make weblogo\n",
    "    if filter_count > 0:\n",
    "        weblogo_cmd = 'weblogo %s < %s.fa > %s.eps' % (weblogo_opts, out_prefix, out_prefix)\n",
    "        subprocess.call(weblogo_cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5465afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_add(meme_out, f, filter_pwm, nsites, trim_filters=False):\n",
    "    ''' Print a filter to the growing MEME file\n",
    "    Attrs:\n",
    "        meme_out : open file\n",
    "        f (int) : filter index #\n",
    "        filter_pwm (array) : filter PWM array\n",
    "        nsites (int) : number of filter sites\n",
    "    '''\n",
    "    if not trim_filters:\n",
    "        ic_start = 0\n",
    "        ic_end = filter_pwm.shape[0]-1\n",
    "    else:\n",
    "        ic_t = 0.2\n",
    "\n",
    "        # trim PWM of uninformative prefix\n",
    "        ic_start = 0\n",
    "        while ic_start < filter_pwm.shape[0] and info_content(filter_pwm[ic_start:ic_start+1]) < ic_t:\n",
    "            ic_start += 1\n",
    "\n",
    "        # trim PWM of uninformative suffix\n",
    "        ic_end = filter_pwm.shape[0]-1\n",
    "        while ic_end >= 0 and info_content(filter_pwm[ic_end:ic_end+1]) < ic_t:\n",
    "            ic_end -= 1\n",
    "\n",
    "    if ic_start < ic_end:\n",
    "        print('MOTIF filter%d' % f, file=meme_out)\n",
    "        print('letter-probability matrix: alength= 4 w= %d nsites= %d' % (ic_end-ic_start+1, nsites), file=meme_out)\n",
    "\n",
    "        for i in range(ic_start, ic_end+1):\n",
    "            print( '%.4f %.4f %.4f %.4f' % tuple(filter_pwm[i]), file=meme_out)\n",
    "\n",
    "\n",
    "        print( '', file=meme_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2a5d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_heat(param_matrix, out_pdf):\n",
    "    param_range = abs(param_matrix).max()\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(param_matrix.shape[1], 4))\n",
    "    sns.heatmap(param_matrix, cmap='PRGn', linewidths=0.2, vmin=-param_range, vmax=param_range)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(range(1,param_matrix.shape[1]+1))\n",
    "    ax.set_yticklabels('ACGT', rotation='horizontal') # , size=10)\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc3e02ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_filters(num_filters, tomtom_file, meme_db_file):\n",
    "    ''' Name the filters using Tomtom matches.\n",
    "    Attrs:\n",
    "        num_filters (int) : total number of filters\n",
    "        tomtom_file (str) : filename of Tomtom output table.\n",
    "        meme_db_file (str) : filename of MEME db\n",
    "    Returns:\n",
    "        filter_names [str] :\n",
    "    '''\n",
    "    # name by number\n",
    "    filter_names = ['f%d'%fi for fi in range(num_filters)]\n",
    "\n",
    "    # name by protein\n",
    "    if tomtom_file is not None and meme_db_file is not None:\n",
    "        print(tomtom_file, meme_db_file)\n",
    "        motif_protein = get_motif_proteins(meme_db_file)\n",
    "        # hash motifs and q-value's by filter\n",
    "        filter_motifs = {}\n",
    "\n",
    "        tt_in = open(tomtom_file)\n",
    "        tt_in.readline()\n",
    "        for line in tt_in:\n",
    "            a = line.split()\n",
    "            if a== []:\n",
    "                break\n",
    "            fi = int(a[0][6:])\n",
    "            motif_id = a[1]\n",
    "            qval = float(a[5])\n",
    "\n",
    "            filter_motifs.setdefault(fi,[]).append((qval,motif_id))\n",
    "\n",
    "        tt_in.close()\n",
    "        # assign filter's best match\n",
    "        for fi in filter_motifs:\n",
    "            top_motif = sorted(filter_motifs[fi])[0][1]\n",
    "            filter_names[fi] += '_%s' % motif_protein[top_motif]\n",
    "\n",
    "    return np.array(filter_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9153d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif_proteins(meme_db_file):\n",
    "    ''' Hash motif_id's to protein names using the MEME DB file '''\n",
    "    motif_protein = {}\n",
    "    for line in open(meme_db_file):\n",
    "        a = line.split()\n",
    "        if len(a) > 0 and a[0] == 'MOTIF':\n",
    "            if a[2][0] == '(':\n",
    "                motif_protein[a[1]] = a[2][1:a[2].find(')')]\n",
    "            else:\n",
    "                motif_protein[a[1]] = a[2]\n",
    "    return motif_protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99689ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filter_bed(filter_num,out_dir):\n",
    "    bed = open(out_dir+'/'+filter_num+'.bed','w')\n",
    "    for key in filter_hits[filter_num].keys():\n",
    "        for peak in filter_hits[filter_num][key]:\n",
    "            print('{0} \\t {1} \\t {2}'.format(key.split('=')[1],peak,peak+motif_length),file=bed) \n",
    "    bed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57d47ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif(filter_weights_old, filter_outs, seqs, out_dir):\n",
    "    global filter_hits\n",
    "    filter_hits = {}\n",
    "    filter_weights = []\n",
    "    for x in filter_weights_old:\n",
    "        x = x - np.mean(x,axis = 0)\n",
    "        filter_weights.append(x)\n",
    "        \n",
    "    filter_weights = np.array(filter_weights)\n",
    "    num_filters = filter_weights.shape[0]\n",
    "    filter_size = filter_weights.shape[2]\n",
    "    filters_ic = []\n",
    "    nsites_list = []\n",
    "    meme_out = meme_intro('%s/filters_meme.txt'%(out_dir), seqs)\n",
    "    for f in range(num_filters):\n",
    "        filter_hits['filter_%i'%f] = {}\n",
    "        # plot filter parameters as a heatmap\n",
    "        plot_filter_heat(filter_weights[f,:,:filter_size], '%s/filter%d_heat.pdf' % (out_dir,f))\n",
    "        # plot weblogo of high scoring outputs\n",
    "        plot_filter_logo(filter_outs[:,f,:], filter_size, seqs, '%s/filter%d_logo'%(out_dir,f), 'filter_%i'%f, maxpct_t=0.8)\n",
    "        generate_filter_bed('filter_%i'%f,out_dir)\n",
    "        # make a PWM for the filter\n",
    "        filter_pwm, nsites = make_filter_pwm('%s/filter%d_logo.fa'%(out_dir,f))\n",
    "        nsites_list.append(nsites)\n",
    "        if nsites < 10:\n",
    "            # no information\n",
    "            filters_ic.append(0)\n",
    "        else:\n",
    "            # compute and save information content\n",
    "            filters_ic.append(info_content(filter_pwm))\n",
    "\n",
    "            # add to the meme motif file\n",
    "            meme_add(meme_out, f, filter_pwm, nsites, False)\n",
    "    pd.DataFrame(filter_hits).to_csv('%s/indices.csv'%out_dir)\n",
    "    meme_out.close()    \n",
    "    subprocess.call('../meme-5.5.2/src/tomtom -dist pearson -thresh 0.05 -oc %s/tomtom %s/filters_meme.txt %s' % (out_dir, out_dir, 'Motif_database/YEASTRACT_20130918.meme'), shell=True)\n",
    "    subprocess.call('cp %s/tomtom/tomtom.tsv %s/tomtom/tomtom.txt' %(out_dir, out_dir), shell=True)\n",
    "    filter_names = name_filters(num_filters, '%s/tomtom/tomtom.txt'%out_dir, 'Motif_database/YEASTRACT_20130918.meme')\n",
    "    \n",
    "    table_out = open('%s/table.txt'%out_dir, 'w')\n",
    "\n",
    "    # print header for later panda reading\n",
    "    table = PrettyTable([\"Filter\", \"consensus\",\"annotation\",\"ic\",'mean', 'std',\"nsites\"])\n",
    "    header_cols = ('', 'consensus', 'annotation', 'ic', 'mean', 'std',\"nsites\")\n",
    "    print('%3s  %24s  %10s  %5s  %6s  %6s  %6s' % header_cols, file=table_out)\n",
    "    \n",
    "    for f in range(num_filters):\n",
    "        # collapse to a consensus motif\n",
    "        consensus = filter_motif(filter_weights[f,:,:])\n",
    "\n",
    "        # grab annotation\n",
    "        annotation = '.'\n",
    "        name_pieces = filter_names[f].split('_')\n",
    "        if len(name_pieces) > 1:\n",
    "            annotation = name_pieces[1]\n",
    "        # plot density of filter output scores\n",
    "        fmean, fstd = plot_score_density(np.ravel(filter_outs[: , f , :]), '%s/filter%d_dens.pdf' % (out_dir,f))\n",
    "\n",
    "        row_cols = (f, consensus, annotation, filters_ic[f], fmean, fstd,nsites_list[f])\n",
    "        table.add_row(list(row_cols))\n",
    "        print( '%-3d  %19s  %10s  %5.2f  %6.4f  %6.4f %4i' % row_cols, file=table_out)\n",
    "        \n",
    "    table_out.close()\n",
    "    print(table.get_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01a32e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb6ccaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir\n",
    "data_dir = 'Data'\n",
    "data_file = data_dir+'/S288C_reference_sequence_R64-3-1_20210421.fsa'\n",
    "peak_file = [data_dir+'/Condensin_peaks_Log.bed',data_dir+'/Condensin_peaks_quiescence.bed']\n",
    "# peak_file can be a list of bed files, it will depend on include_dinuc variable whether to assign to each file a different label or to assign them label 1 and the shuffeled seq will be labeled 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "618cb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global file_extension\n",
    "global model_dir\n",
    "global results_dir\n",
    "global verbose\n",
    "global new\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "file_extension = peak_file.split('.')[0].split('_')[-1] if type(peak_file)==str else 'mix'\n",
    "model_dir = 'model_'+file_extension\n",
    "results_dir = 'results_'+file_extension\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "if not os.path.exists(results_dir+'/tomtom'):\n",
    "    os.mkdir(results_dir+'/tomtom')\n",
    "    \n",
    "verbose = False\n",
    "new = True  #assign False if a pretrained model already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c73b4a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4248, 472, 4720, 610)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 500\n",
    "motif_length = 24\n",
    "cross_chrom = True\n",
    "include_dinuc = True\n",
    "calib_data,valid_data,train_data,test_data,peak_sequences_train,peak_sequences_test = extract_data(data_file,peak_file,motif_length,\n",
    "                                                                                                   seq_length,cross_chrom,include_dinuc)\n",
    "len(calib_data),len(valid_data),len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50fc972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dataset=dataset(calib_data)\n",
    "valid_dataset=dataset(valid_data)\n",
    "train_dataset=dataset(train_data)\n",
    "test_dataset=dataset(test_data)\n",
    "batch_size = 64\n",
    "calib_loader = DataLoader(dataset=calib_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "734a0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 out of 30\n",
      "early stopping at epoch  16\n",
      "model 2 out of 30\n",
      "early stopping at epoch  162\n",
      "model 3 out of 30\n",
      "no early stopping\n",
      "model 4 out of 30\n",
      "early stopping at epoch  273\n",
      "model 5 out of 30\n",
      "early stopping at epoch  138\n",
      "model 6 out of 30\n",
      "no early stopping\n",
      "model 7 out of 30\n",
      "early stopping at epoch  7\n",
      "model 8 out of 30\n",
      "no early stopping\n",
      "model 9 out of 30\n",
      "early stopping at epoch  170\n",
      "model 10 out of 30\n",
      "early stopping at epoch  145\n",
      "model 11 out of 30\n",
      "early stopping at epoch  24\n",
      "model 12 out of 30\n",
      "early stopping at epoch  16\n",
      "model 13 out of 30\n",
      "early stopping at epoch  29\n",
      "model 14 out of 30\n",
      "early stopping at epoch  18\n",
      "model 15 out of 30\n",
      "no early stopping\n",
      "model 16 out of 30\n",
      "early stopping at epoch  179\n",
      "model 17 out of 30\n",
      "early stopping at epoch  31\n",
      "model 18 out of 30\n",
      "early stopping at epoch  5\n",
      "model 19 out of 30\n",
      "no early stopping\n",
      "model 20 out of 30\n",
      "early stopping at epoch  254\n",
      "model 21 out of 30\n",
      "early stopping at epoch  20\n",
      "model 22 out of 30\n",
      "early stopping at epoch  4\n",
      "model 23 out of 30\n",
      "early stopping at epoch  22\n",
      "model 24 out of 30\n",
      "early stopping at epoch  217\n",
      "model 25 out of 30\n",
      "early stopping at epoch  140\n",
      "model 26 out of 30\n",
      "early stopping at epoch  24\n",
      "model 27 out of 30\n",
      "early stopping at epoch  12\n",
      "model 28 out of 30\n",
      "no early stopping\n",
      "model 29 out of 30\n",
      "no early stopping\n",
      "model 30 out of 30\n",
      "early stopping at epoch  13\n"
     ]
    }
   ],
   "source": [
    "num_motif_list = [30,40,60]\n",
    "num_conv_layers_list = [1,2]\n",
    "dropprob_list = [0, 0.15, 0.3]\n",
    "learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "max_num_models = 30\n",
    "maxepochs = 500\n",
    "epochs_for_early_stop = 50\n",
    "best_hyperparameters,results = Calibrate_model(calib_loader,valid_loader, num_motif_list, num_conv_layers_list , dropprob_list,\n",
    "                                               learning_rate_list, max_num_models=max_num_models, maxepochs=maxepochs,\n",
    "                                               epochs_for_early_stop=epochs_for_early_stop,motif_len=motif_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab6eecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_conv_layers</th>\n",
       "      <th>num_motif</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>138</td>\n",
       "      <td>0.950697</td>\n",
       "      <td>0.950778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>254</td>\n",
       "      <td>0.947195</td>\n",
       "      <td>0.952806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>273</td>\n",
       "      <td>0.943856</td>\n",
       "      <td>0.949412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>145</td>\n",
       "      <td>0.942366</td>\n",
       "      <td>0.946570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>162</td>\n",
       "      <td>0.940786</td>\n",
       "      <td>0.943991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  num_conv_layers num_motif  Dropout  Learning Rate epochs     AUROC     AUPRC\n",
       "0               2        60     0.00         0.0001    138  0.950697  0.950778\n",
       "0               1        40     0.15         0.0001    254  0.947195  0.952806\n",
       "0               1        30     0.00         0.0001    273  0.943856  0.949412\n",
       "0               2        40     0.30         0.0001    145  0.942366  0.946570\n",
       "0               1        60     0.00         0.0001    162  0.940786  0.943991"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "322d7eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epochs': 138,\n",
       " 'best_num_motif': 60,\n",
       " 'best_num_conv_layers': 2,\n",
       " 'best_dropprob': 0,\n",
       " 'best_l_rate': 0.0001}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "314fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no early stopping\n",
      "+------------------------+------------+\n",
      "|        Modules         | Parameters |\n",
      "+------------------------+------------+\n",
      "|  conv_layer.0.weight   |    5760    |\n",
      "|   conv_layer.0.bias    |     60     |\n",
      "|  conv_layer.3.weight   |   64800    |\n",
      "|   conv_layer.3.bias    |     90     |\n",
      "| project.to_attn_logits |    8100    |\n",
      "|  classifier.0.weight   |    8100    |\n",
      "|   classifier.0.bias    |     90     |\n",
      "|  classifier.3.weight   |     90     |\n",
      "|   classifier.3.bias    |     1      |\n",
      "+------------------------+------------+\n",
      "\n",
      " Total Trainable Params: 87091\n"
     ]
    }
   ],
   "source": [
    "maxepochs = best_hyperparameters['best_epochs']\n",
    "num_motif = best_hyperparameters['best_num_motif']\n",
    "num_conv_layers = best_hyperparameters['best_num_conv_layers']\n",
    "dropprob = best_hyperparameters['best_dropprob']\n",
    "l_rate = best_hyperparameters['best_l_rate']\n",
    "epochs_for_early_stop = 0\n",
    "model = Network(num_motif , motif_length , num_conv_layers , dropprob).to(device)\n",
    "best_model,epochs = Train_model(model,train_loader,valid_loader,l_rate ,maxepochs,\n",
    "                                                          epochs_for_early_stop,save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fda8ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9756409567320613, 0.9768832059548929)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc,auprc = Test_model(best_model,test_loader)\n",
    "auroc,auprc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca16a8d",
   "metadata": {},
   "source": [
    "# Motif extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "422fe959",
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_sequences=peak_sequences_train.copy()\n",
    "motif_sequences.extend(peak_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "227ac032",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_Weights = best_model.conv[0].weight.detach().cpu().numpy()\n",
    "filter_bias = best_model.conv[0].bias.detach().cpu().numpy()\n",
    "motif_data=generate_onehot_data(motif_sequences,motif_length=1,label=1,include_dinuc=False)\n",
    "motif_dataset=dataset(motif_data)\n",
    "motif_loader = DataLoader(dataset=motif_dataset,batch_size=batch_size,shuffle=False)\n",
    "out_model = conv_output(filter_Weights,filter_bias,device)\n",
    "filter_output = return_filter_outputs(out_model,motif_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17059f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The output directory 'results_mix/tomtom' already exists.\n",
      "Its contents will be overwritten.\n",
      "Processing query 1 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 2 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 3 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00022\n",
      "#   Estimated pi_0=1\n",
      "Processing query 4 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00276\n",
      "#   Estimated pi_0=1\n",
      "Processing query 5 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00365\n",
      "#   Estimated pi_0=1\n",
      "Processing query 6 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 7 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00091\n",
      "#   Estimated pi_0=1\n",
      "Processing query 8 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 9 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 10 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 11 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n",
      "Processing query 12 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00365\n",
      "#   Estimated pi_0=1\n",
      "Processing query 13 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 14 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n",
      "Processing query 15 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 16 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n",
      "Processing query 17 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 18 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.998847\n",
      "#   Estimated pi_0=0.998847\n",
      "Processing query 19 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00228\n",
      "#   Estimated pi_0=1\n",
      "Processing query 20 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.995414\n",
      "#   Estimated pi_0=0.995414\n",
      "Processing query 21 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.998847\n",
      "#   Estimated pi_0=0.998847\n",
      "Processing query 22 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.999752\n",
      "#   Estimated pi_0=1\n",
      "Processing query 23 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00365\n",
      "#   Estimated pi_0=1\n",
      "Processing query 24 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00091\n",
      "#   Estimated pi_0=1\n",
      "Processing query 25 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00503\n",
      "#   Estimated pi_0=1\n",
      "Processing query 26 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00365\n",
      "#   Estimated pi_0=1\n",
      "Processing query 27 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00503\n",
      "#   Estimated pi_0=1\n",
      "Processing query 28 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00503\n",
      "#   Estimated pi_0=1\n",
      "Processing query 29 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 30 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n",
      "Processing query 31 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.993355\n",
      "#   Estimated pi_0=0.993355\n",
      "Processing query 32 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 33 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00091\n",
      "#   Estimated pi_0=1\n",
      "Processing query 34 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.999279\n",
      "#   Estimated pi_0=1\n",
      "Processing query 35 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.994041\n",
      "#   Estimated pi_0=0.994041\n",
      "Processing query 36 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.983897\n",
      "#   Estimated pi_0=0.983897\n",
      "Processing query 37 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 38 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00228\n",
      "#   Estimated pi_0=1\n",
      "Processing query 39 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00113\n",
      "#   Estimated pi_0=1\n",
      "Processing query 40 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.99816\n",
      "#   Estimated pi_0=0.99816\n",
      "Processing query 41 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.996787\n",
      "#   Estimated pi_0=0.996787\n",
      "Processing query 42 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 43 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.998847\n",
      "#   Estimated pi_0=0.998847\n",
      "Processing query 44 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00297\n",
      "#   Estimated pi_0=1\n",
      "Processing query 45 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.994728\n",
      "#   Estimated pi_0=0.994728\n",
      "Processing query 46 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00091\n",
      "#   Estimated pi_0=1\n",
      "Processing query 47 out of 60 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00503\n",
      "#   Estimated pi_0=1\n",
      "Processing query 48 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 49 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n",
      "Processing query 50 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.995414\n",
      "#   Estimated pi_0=0.995414\n",
      "Processing query 51 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00228\n",
      "#   Estimated pi_0=1\n",
      "Processing query 52 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.999533\n",
      "#   Estimated pi_0=0.999533\n",
      "Processing query 53 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00434\n",
      "#   Estimated pi_0=1\n",
      "Processing query 54 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00228\n",
      "#   Estimated pi_0=1\n",
      "Processing query 55 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00365\n",
      "#   Estimated pi_0=1\n",
      "Processing query 56 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00022\n",
      "#   Estimated pi_0=1\n",
      "Processing query 57 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.990437\n",
      "#   Estimated pi_0=0.991472\n",
      "Processing query 58 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 0.999062\n",
      "#   Estimated pi_0=0.999533\n",
      "Processing query 59 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00067\n",
      "#   Estimated pi_0=1\n",
      "Processing query 60 out of 60 \n",
      "# Computing q-values.\n",
      "#   Estimating pi_0 from all 1464 observed p-values.\n",
      "#   Estimating pi_0.\n",
      "# Minimal pi_zero = 1.00159\n",
      "#   Estimated pi_0=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_mix/tomtom/tomtom.txt Motif_database/YEASTRACT_20130918.meme\n",
      "+--------+--------------------------+------------+--------------------+-------------+------------+--------+\n",
      "| Filter |        consensus         | annotation |         ic         |     mean    |    std     | nsites |\n",
      "+--------+--------------------------+------------+--------------------+-------------+------------+--------+\n",
      "|   0    | GGGCATACGTTAACCTGCGAGCAT |     .      | 11.350989127442071 |  0.23894928 | 0.25270543 |   38   |\n",
      "|   1    | TTAACACGCGGTAAGCGCTTAACC |     .      | 10.181426068901843 |  0.16367914 | 0.19221641 |  147   |\n",
      "|   2    | ACACATACTTATCGTTTTTTTTGC |   Hsf1p    | 19.859827827416787 |   0.29639   | 0.31235033 |  457   |\n",
      "|   3    | AGTGCGCCGCGTACTCACTGATTA |     .      | 9.610126126135519  |  0.12056698 | 0.18135278 |   97   |\n",
      "|   4    | CTGCTGTTGGAGCAGCAAAAGCTG |     .      | 14.195094272291023 |  0.4280269  | 0.29944867 |   97   |\n",
      "|   5    | GGCGTAACGCTCGGCTAAAGACGC |     .      | 9.380273054564533  |  0.05172892 | 0.11292083 |   17   |\n",
      "|   6    | TCATTATTCATCATACTCCATCTG |     .      | 12.459239555850944 |  0.1767315  | 0.22164254 |  259   |\n",
      "|   7    | CTGACAGACGGTTCCTCCTCAACG |     .      | 11.098216697031502 |  0.2407438  | 0.26307508 |  121   |\n",
      "|   8    | GCCTTAGAAGTTCGGCTTACCAAC |     .      | 9.732261536438989  |  0.11565799 | 0.16638644 |  163   |\n",
      "|   9    | TTGGGTCCTTAGAAAGCGAACACT |     .      | 10.860344475710164 |  0.16862932 | 0.20420516 |  104   |\n",
      "|   10   | GCATGGGAAAAGGCTGTAGCGGCC |     .      | 12.686885969091694 |  0.14514765 | 0.19645943 |   78   |\n",
      "|   11   | GACAAAGTAGAAGGGGTCCATACC |     .      | 10.713216018483044 |  0.2538921  | 0.24323541 |  283   |\n",
      "|   12   | ATGATCATGAGATGGTTCTTCTTG |     .      | 13.39067993583961  |  0.10017982 | 0.16639438 |   78   |\n",
      "|   13   | CATGTTCTTCCGCGCACTTTGATG |     .      | 13.353868283822198 | 0.082551986 | 0.1439179  |   72   |\n",
      "|   14   | AAGCGTTAAGAGGTATATTTCTGG |     .      | 12.273336659957232 |  0.14227709 | 0.17888455 |   88   |\n",
      "|   15   | GGAATGAAACCCGCGTAATCCCTA |     .      | 10.536426096428883 |  0.13493063 | 0.19332628 |   74   |\n",
      "|   16   | AGAGACGCTAGCTGGAGCGATTGT |     .      | 10.374346038868897 |  0.34324208 | 0.3112425  |   55   |\n",
      "|   17   | CAATCACTAACATAATACGCTACG |     .      | 12.55877851204157  | 0.114685036 | 0.16483648 |  190   |\n",
      "|   18   | TACTGTTTATACATAGCCGACGTG |     .      | 12.179277593797424 |  0.27134243 | 0.23708752 |  192   |\n",
      "|   19   | CGAGATGTATATACATCCCTTTTC |     .      | 15.349058430867853 |  0.34608445 | 0.33528885 |  127   |\n",
      "|   20   | TGCTCAGATGCAGTCGCGGTACGG |     .      | 10.957310834176248 |  0.10722582 | 0.17956719 |  100   |\n",
      "|   21   | CCTGATTTCGGCTATTTAACTGCT |     .      | 10.989330024034526 |  0.23589498 | 0.22245182 |  125   |\n",
      "|   22   | CCTCACCCCGTTGCCAGTCAGTCG |     .      | 10.810955491169468 |  0.11712132 | 0.18192415 |  125   |\n",
      "|   23   | CTACATCCGGCTCTCCTCTCTCCT |     .      | 12.036155907802636 |   0.308465  | 0.2704547  |   42   |\n",
      "|   24   | GGCCGACCTAACGACTTGGTTCCC |     .      | 9.629563068907485  |  0.1232738  | 0.17914009 |   88   |\n",
      "|   25   | CGGTAAAGAGTCAGTTGACAAGTC |     .      | 14.800801108470989 |  0.20029965 | 0.2168574  |   44   |\n",
      "|   26   | TTACTCGCCTAGCCTCGATTGGCT |     .      | 10.102399701135234 |  0.24658771 | 0.2229252  |  276   |\n",
      "|   27   | AGAAACAGTTGCTGCCCCTGCTGC |     .      | 12.762518784159353 |  0.37101826 | 0.30583254 |   35   |\n",
      "|   28   | TTGCGAATGCCGTTAAGGACGACA |     .      | 11.480450279516912 |   0.281028  | 0.27011725 |   19   |\n",
      "|   29   | TTCGTTGGGAGTCGTTCGTCATTT |     .      | 11.824779059368048 |   0.267527  | 0.24675605 |  168   |\n",
      "|   30   | ATTTTCTCTAGTCTATATATATTA |   Hsf1p    | 23.747066801454135 |  0.19430164 | 0.2916557  |  229   |\n",
      "|   31   | AGCATGACTAAAGGGTGACGCTCA |     .      | 10.175985476738555 |  0.17478229 | 0.20666558 |  148   |\n",
      "|   32   | ACTCCGCCACCACGGAGGAACTAC |     .      | 10.747692503049658 |  0.23822281 | 0.22532627 |  122   |\n",
      "|   33   | GGGGCGTCGGCCGGGCGACGGGTC |     .      | 11.69602981066554  |  0.3724696  | 0.2993404  |  160   |\n",
      "|   34   | ACATGTATATGGTTGTGTCGTATA |   Nhp6bp   | 15.592583973947322 |  0.2565024  | 0.25592443 |  369   |\n",
      "|   35   | TCGGTCTTAGAGCGGCGGGAGCGC |     .      | 8.911403007488339  |  0.2388657  | 0.25123727 |  172   |\n",
      "|   36   | TGATAGTAGGTAACTCGCAATAAG |     .      | 12.71531011578665  |  0.13118777 | 0.18134102 |   50   |\n",
      "|   37   | GGCAGATCTCGGGTTAAACGCGTT |     .      | 10.225529420006353 |  0.08759702 | 0.14643906 |  122   |\n",
      "|   38   | TAGCCCGGACTCGCGGGACGGCAG |     .      | 9.499567477929366  |  0.12844075 | 0.18017401 |   59   |\n",
      "|   39   | TATATACATATGCACGTACCAGAA |   Nhp6ap   | 18.19115160191788  |  0.1465506  | 0.23081686 |   92   |\n",
      "|   40   | ACAGCTATGTATCGTATACGGATA |   Nhp6bp   | 13.946422049708247 |  0.30642548 | 0.24642919 |  157   |\n",
      "|   41   | CTCCGCGAAGGATCACGAATACTT |     .      | 11.618215379078634 |  0.23869908 | 0.29443064 |   24   |\n",
      "|   42   | TTTGCTTTTGTTCTAGCTTGTCGT |     .      | 12.920661871453687 |  0.27297044 | 0.26135522 |  187   |\n",
      "|   43   | GCTGCATGATAATCGCACAAAAGG |     .      | 11.816147023397413 | 0.104815364 | 0.16301112 |   84   |\n",
      "|   44   | TTATCATAATTTAACCTTAAAAGC |   Sum1p    | 15.452770665540525 |  0.14631593 | 0.18702877 |  111   |\n",
      "|   45   | GCCGCAGAGACGGGCTCACTACCG |     .      | 8.924036161115705  |  0.25850174 | 0.26264444 |  257   |\n",
      "|   46   | TAGCGTCGGCTCAGCTTACTGACC |     .      | 10.163644915240573 |  0.12521563 | 0.17690566 |   92   |\n",
      "|   47   | GTTTTTATACCTCGACTAGCTACT |     .      | 12.653717154338215 |  0.1640267  | 0.2017841  |   93   |\n",
      "|   48   | GCGATCGCGCGTTCAACGGATGCT |     .      | 11.410793012808714 |  0.32735002 | 0.2998836  |   49   |\n",
      "|   49   | TACTATACTATATTACATACATAT |   Nhp6bp   | 17.954925547856664 |  0.1969673  | 0.25540483 |  149   |\n",
      "|   50   | AGATCCCCTCTGCGCTAGACGCGT |     .      | 10.187112974736017 |  0.10636265 | 0.16886747 |  113   |\n",
      "|   51   | AAGTCTTAATTCTTAGTCGGGCCT |     .      | 10.16489092879433  |  0.25620294 | 0.2501172  |  240   |\n",
      "|   52   | TGTCACTGCCGCCGCTGCCGCCGC |     .      | 17.76485861686676  |  0.1494006  | 0.20379806 |   43   |\n",
      "|   53   | GTGCCGCAGTCTGTGCCACCGCCG |     .      | 11.500168435485765 |  0.40521333 | 0.3094063  |  131   |\n",
      "|   54   | GTTGCAGTTGCACGAGCCCGAGAA |     .      |  11.1503303978819  |  0.3098809  | 0.27233618 |  106   |\n",
      "|   55   | CGGTCCGCCTTCCCGGGCTTGCCA |     .      | 10.242523486508961 |  0.3417616  |  0.277202  |  109   |\n",
      "|   56   | AAGACGAGAACCCGTGGCGAGTGT |     .      | 11.063308167807614 |  0.19080408 | 0.21179534 |   77   |\n",
      "|   57   | CTGCTTCGGTCCCTGCGCCGATGG |   Rfx1p    | 10.98599324560521  |  0.1257735  | 0.18977131 |  121   |\n",
      "|   58   | GTATACGGACATGAATGAAGAAAG |     .      | 14.396815962724284 |  0.1894229  | 0.21386233 |   80   |\n",
      "|   59   | GCGAATTTAGAACTTAGCGTTTAA |     .      | 12.619185303766221 |  0.07609516 | 0.13336554 |   58   |\n",
      "+--------+--------------------------+------------+--------------------+-------------+------------+--------+\n"
     ]
    }
   ],
   "source": [
    "get_motif(filter_Weights,filter_output,motif_sequences,results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843548e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
