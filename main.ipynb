{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea054c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math \n",
    "import random\n",
    "import gzip\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import einsum\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e64639",
   "metadata": {},
   "source": [
    "# DATA Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01ff4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chrom_train_test(peaks,Chr_dict):\n",
    "    peak_chromosomes = {}\n",
    "    n_peaks = 0\n",
    "    chrom_train = []\n",
    "    chrom_test = []\n",
    "    for line in peaks:\n",
    "        split_line = 'chromosome='+line.split('\\t')[0]\n",
    "        peak_chromosomes[split_line] = peak_chromosomes.get(split_line,0) + 1\n",
    "        n_peaks+=1\n",
    "    size_test = int(n_peaks/10)\n",
    "    npeaks_test = 0\n",
    "    while npeaks_test<size_test:\n",
    "        chrom = random.choice(list(Chr_dict.keys()))\n",
    "        npeaks_test += peak_chromosomes[chrom]\n",
    "        chrom_test.append(chrom)\n",
    "        if npeaks_test>int(n_peaks*1.3/10):\n",
    "            npeaks_test = 0\n",
    "            chrom_test = []\n",
    "    for chrom in Chr_dict.keys():\n",
    "        if chrom not in chrom_test:\n",
    "            chrom_train.append(chrom)\n",
    "    return chrom_train,chrom_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4831bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genome_data(data_file):\n",
    "    data=open(data_file).read()\n",
    "    chromosomes_data = data.split('>')[1:]\n",
    "    return (chromosomes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a84bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Read_bed_file(chromosomes_data,peak_file,seq_length,Chr_dict,cross_chrom):\n",
    "    peaks = open(peak_file).readlines()\n",
    "    peak_sequences = []\n",
    "    if cross_chrom:\n",
    "        peak_sequences_train,peak_sequences_test = [],[]\n",
    "        chrom_train , chrom_test = generate_chrom_train_test(peaks,Chr_dict)\n",
    "    for peak in peaks:\n",
    "        peak_split = peak.split('\\t')\n",
    "        Chr = 'chromosome='+str(peak_split[0])\n",
    "        if Chr in Chr_dict:\n",
    "            chrom_seq = Chr_dict[Chr]\n",
    "            n = len(chrom_seq)\n",
    "            start_idx = max(int(peak_split[1])-seq_length//2,0)\n",
    "            end_idx = min(int(len(chrom_seq)), start_idx+seq_length)\n",
    "            if end_idx == len(chrom_seq):\n",
    "                start_idx = end_idx - seq_length\n",
    "            header = Chr+':{0}-{1}'.format(start_idx,end_idx)\n",
    "            if not cross_chrom:\n",
    "                peak_sequences.append([header,chrom_seq[start_idx:end_idx]])\n",
    "            else:\n",
    "                if Chr in chrom_train:\n",
    "                    peak_sequences_train.append([header,chrom_seq[start_idx:end_idx]])\n",
    "                else:\n",
    "                    peak_sequences_test.append([header,chrom_seq[start_idx:end_idx]])\n",
    "    if not cross_chrom:\n",
    "        size=int(len(peak_sequences)/10)\n",
    "        peak_sequences_train = peak_sequences[:9*size]\n",
    "        peak_sequences_test = peak_sequences[9*size:]\n",
    "    return (peak_sequences_train,peak_sequences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05d07964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dinucshuffle(sequence):\n",
    "    b=[sequence[i:i+2] for i in range(0, len(sequence), 2)]\n",
    "    random.shuffle(b)\n",
    "    d=''.join([str(x) for x in b])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "584b788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqtopad(sequence, motif_len):\n",
    "    rows=len(sequence)+2*motif_len-2\n",
    "    S=np.empty([rows,4])\n",
    "    base=['A', 'C', 'G', 'T']\n",
    "    for i in range(rows):\n",
    "        for j in range(4):\n",
    "            if (i-motif_len+1<len(sequence) and sequence[i-motif_len+1]=='N' \n",
    "                or i<motif_len-1 or i>len(sequence)+motif_len-2):\n",
    "                S[i,j]=np.float32(0.25)\n",
    "            elif sequence[i-motif_len+1]==base[j]:\n",
    "                S[i,j]=np.float32(1)\n",
    "            else:\n",
    "                S[i,j]=np.float32(0)\n",
    "    return np.transpose(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b0f38a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_data(peak_sequences,motif_length,label,include_dinuc):\n",
    "    alldata = []\n",
    "    for header,seq in peak_sequences:\n",
    "        alldata.append([header,seq,seqtopad(seq,motif_length),[int(label)]])#\n",
    "        if include_dinuc:\n",
    "            shuff_seq = dinucshuffle(seq)\n",
    "            alldata.append([header,shuff_seq,seqtopad(shuff_seq,motif_length),[0]])#\n",
    "    return (alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d582c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(data_file,peak_file,motif_length=24,seq_length=150,cross_chrom=False,include_dinuc=True,Chr_dict=None):\n",
    "    chromosomes_data = genome_data(data_file)\n",
    "    if Chr_dict==None:\n",
    "        Chr_dict = {}\n",
    "        for chrom_data in chromosomes_data:\n",
    "            ref = chrom_data.split('\\n')[0].split(' ')[-1][1:-1]\n",
    "            Chr_dict[ref]=''.join(chrom_data.split('\\n')[1:])\n",
    "        Chr_dict['chromosome=Mito'] = Chr_dict.pop('top=circular')\n",
    "    if type(peak_file) == str:\n",
    "        peak_sequences_train,peak_sequences_test = Read_bed_file(chromosomes_data,peak_file,seq_length,Chr_dict,cross_chrom)\n",
    "        train_data = generate_onehot_data(peak_sequences_train,motif_length,1,include_dinuc)\n",
    "        test_data = generate_onehot_data(peak_sequences_test,motif_length,1,include_dinuc)\n",
    "        random.shuffle(train_data)\n",
    "        size=int(len(train_data)/10)\n",
    "        calib_data=train_data[:9*size]\n",
    "        valid_data=train_data[9*size:]\n",
    "    elif type(peak_file) == list:\n",
    "        include_dinuc = False\n",
    "        peak_sequences_train,peak_sequences_test = [],[]\n",
    "        train_data,test_data = [],[]\n",
    "        for i in range (len(peak_file)):\n",
    "            peak_sequences_train_temp,peak_sequences_test_temp = Read_bed_file(chromosomes_data,peak_file[i],seq_length,Chr_dict,cross_chrom)\n",
    "            peak_sequences_train.extend(peak_sequences_train_temp)\n",
    "            peak_sequences_test.extend(peak_sequences_test_temp)\n",
    "            train_data.extend(generate_onehot_data(peak_sequences_train_temp,motif_length,i,include_dinuc))\n",
    "            test_data.extend(generate_onehot_data(peak_sequences_test_temp,motif_length,i,include_dinuc))\n",
    "        random.shuffle(train_data)\n",
    "        random.shuffle(test_data)\n",
    "        size=int(len(train_data)/10)\n",
    "        calib_data=train_data[:9*size]\n",
    "        valid_data=train_data[9*size:]\n",
    "    return calib_data,valid_data,train_data,test_data,peak_sequences_train,peak_sequences_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6057d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, xy=None):\n",
    "        self.header=[el[0] for el in xy]\n",
    "        self.seq =[el[1] for el in xy ]\n",
    "        self.x_data=np.asarray([el[2] for el in xy],dtype=np.float32)\n",
    "        self.y_data =np.asarray([el[3] for el in xy ],dtype=np.float32)\n",
    "        self.x_data = torch.from_numpy(self.x_data)\n",
    "        self.y_data = torch.from_numpy(self.y_data)\n",
    "        self.length=len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.header[index],self.seq[index],self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fef79b14",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70a7e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.to_attn_logits = nn.Parameter(torch.eye(dim)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_logits = einsum('b n d, d e -> b n e', x, self.to_attn_logits) \n",
    "        attn = attn_logits.softmax(dim = -2) \n",
    "        return (x * attn).sum(dim = -2).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12e31aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, num_motif , motif_len , num_conv_layers , dropprob):\n",
    "        super(Network, self).__init__()\n",
    "        self.num_motif = num_motif\n",
    "        self.conv = [nn.Conv1d(4, num_motif, kernel_size=motif_len),nn.ReLU(inplace=True)]\n",
    "        in_channels = num_motif\n",
    "        for i in range (num_conv_layers-1):\n",
    "            motif_len = motif_len//2\n",
    "            self.conv.append(nn.MaxPool1d(kernel_size=3))\n",
    "            self.conv.append(nn.Conv1d(in_channels, int(1.5*in_channels), kernel_size=motif_len))\n",
    "            self.conv.append(nn.ReLU(inplace=True))\n",
    "            in_channels = int(1.5*in_channels)\n",
    "        self.conv_layer = nn.Sequential(*self.conv)\n",
    "        self.project = AttentionPool(in_channels)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_channels , in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropprob, inplace=False),\n",
    "            nn.Linear(in_channels, 1),\n",
    "            nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x= x.permute(0, 2, 1)\n",
    "        x = self.project(x)\n",
    "        predict = self.classifier(x)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fb2ef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_output(nn.Module):\n",
    "    def __init__(self,filter_weights,filter_bias,device):\n",
    "        super(conv_output, self).__init__()\n",
    "        if type(filter_weights) is np.ndarray:\n",
    "            self.filter_weights =  torch.from_numpy(filter_weights.astype(np.float32)).to(device)\n",
    "        else :\n",
    "            self.filter_weights = filter_weights.to(device)\n",
    "        if type(filter_bias) is np.ndarray:\n",
    "            self.filter_bias =  torch.from_numpy(filter_bias.astype(np.float32)).to(device)\n",
    "        else :\n",
    "            self.filter_bias = filter_bias.to(device)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=F.conv1d(x, self.filter_weights, bias=self.filter_bias, stride=1, padding=0)\n",
    "        out=x.clamp(min=0)\n",
    "        return (out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0649a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### printing parameters ------------------\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table.get_string())\n",
    "    print(f\"\\n Total Trainable Params: {total_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c20d6ce6",
   "metadata": {},
   "source": [
    "# Calib - Train - Test functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79573514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(model,train_loader,valid_loader, l_rate=0.01 , maxepochs=100,epochs_for_early_stop=0,save_model=False):\n",
    "    best_model = None\n",
    "    best_loss = np.inf\n",
    "    counter = 0\n",
    "    nepochs=0\n",
    "    valid_losses =[]\n",
    "    train_losses = []\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=l_rate,weight_decay=1e-05)\n",
    "    criterion = nn.BCELoss(reduction='mean')\n",
    "    while nepochs<maxepochs:\n",
    "        model.train()\n",
    "        train_loss=0\n",
    "        for i, (header, seq, data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)#\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss+=loss.item()\n",
    "        if verbose:\n",
    "            print('Model trained for {0} epochs out of {2}. Training loss is {1}'.format(nepochs+1,loss.item(),maxepochs))\n",
    "        train_losses.append(train_loss/(i+1))\n",
    "        with torch.no_grad():\n",
    "            model.eval() \n",
    "            valid_loss=0\n",
    "            for i, (header, seq, data, target) in enumerate(valid_loader):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = model(data)\n",
    "                loss = F.binary_cross_entropy(output, target)#\n",
    "                valid_loss+=loss.item()\n",
    "            valid_losses.append(valid_loss/(i+1))\n",
    "        counter+=1\n",
    "        nepochs +=1\n",
    "        if epochs_for_early_stop>0:\n",
    "            if valid_losses[-1]<best_loss:\n",
    "                if verbose:\n",
    "                    print('Validation loss decreased from {0} to {1}'.format(best_loss,valid_losses[-1]))\n",
    "                best_loss = valid_losses[-1]\n",
    "                best_model = model\n",
    "                counter = 0\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print('Counter for early stopping: {0} out of {1}'.format(counter,epochs_for_early_stop))\n",
    "                if counter == epochs_for_early_stop:\n",
    "                    print('early stopping at epoch ', nepochs-counter)\n",
    "                    if save_model:\n",
    "                        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "                        count_parameters(best_model)\n",
    "                    return (best_model,nepochs-counter,train_losses,valid_losses)\n",
    "    print('no early stopping')\n",
    "    if save_model:\n",
    "        torch.save(best_model,model_dir+'/best_model.pkl')\n",
    "        count_parameters(model)\n",
    "    return (model,nepochs,train_losses,valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e481f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_model(model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred_list = []\n",
    "        labels_list = []\n",
    "        for i, (header, seq, data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            pred=output.cpu().detach().numpy().reshape(output.shape[0])\n",
    "            labels=target.cpu().numpy().reshape(output.shape[0])\n",
    "            pred_list.append(pred)\n",
    "            labels_list.append(labels)\n",
    "        labels = np.concatenate(labels_list)\n",
    "        predictions = np.concatenate(pred_list)\n",
    "    auc = metrics.roc_auc_score(labels, predictions)\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(labels, predictions)\n",
    "    prc = (metrics.auc(recall, precision))\n",
    "    if verbose:\n",
    "        print('AUROC on test data ', auc)\n",
    "        print('AUPRC on test data ', prc)\n",
    "    return (auc,prc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e24718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calibrate_model(calib_loader,valid_loader, num_motif_list, num_conv_layers_list , dropprob_list,learning_rate_list, \n",
    "                    max_num_models=40, maxepochs=100,epochs_for_early_stop=0 , motif_len=24 ):\n",
    "    results=pd.DataFrame(columns=['num_conv_layers','num_motif','Dropout','Learning Rate','epochs','AUROC','AUPRC'])\n",
    "    best_AUC = 0\n",
    "    if verbose:\n",
    "        print('Training on ',device)\n",
    "    for number in range(max_num_models):\n",
    "        print('model {0} out of {1}'.format(number+1,max_num_models))\n",
    "        # hyper-parameters\n",
    "        num_motif = random.choice(num_motif_list)\n",
    "        num_conv_layers = random.choice(num_conv_layers_list)\n",
    "        dropprob = random.choice(dropprob_list)\n",
    "        l_rate = random.choice(learning_rate_list)\n",
    "        while ((results['num_conv_layers']==num_conv_layers) & (results['num_motif']==num_motif) \n",
    "               & (results['Dropout']==dropprob) & (results['Learning Rate']==l_rate)).any(): \n",
    "            #if hyperparameters exist in the results dataframe then randomly choose other parameters\n",
    "            num_motif = random.choice(num_motif_list)\n",
    "            num_conv_layers = random.choice(num_conv_layers_list)\n",
    "            dropprob = random.choice(dropprob_list)\n",
    "            l_rate = random.choice(learning_rate_list)\n",
    "        model = Network(num_motif , motif_len , num_conv_layers , dropprob).to(device)#num_conv_layers,dropprob\n",
    "        best_model,epochs,train_losses,valid_losses = Train_model(model,calib_loader,valid_loader,l_rate ,maxepochs,epochs_for_early_stop)\n",
    "        auc,prc = Test_model(best_model,valid_loader)\n",
    "        results=pd.concat([results,pd.DataFrame({'num_conv_layers':num_conv_layers,'num_motif':num_motif,'Dropout':dropprob,\n",
    "                                                 'Learning Rate':l_rate,'epochs':epochs,'AUROC':[auc],'AUPRC':[prc]})])\n",
    "        if auc > best_AUC :\n",
    "            best_AUC = auc\n",
    "            best_epochs = epochs\n",
    "            best_num_motif = num_motif\n",
    "            best_num_conv_layers = num_conv_layers\n",
    "            best_dropprob = dropprob\n",
    "            best_l_rate = l_rate\n",
    "    best_hyperparameters = {'best_epochs': best_epochs,'best_num_motif':best_num_motif,\n",
    "                            'best_num_conv_layers':best_num_conv_layers,'best_dropprob':best_dropprob,'best_l_rate':best_l_rate}\n",
    "    torch.save(best_hyperparameters, model_dir+'/best_hyperpamarameters.pth')\n",
    "    results.sort_values(by='AUROC',ascending=False,inplace=True)\n",
    "    results.to_csv(model_dir+'/calibration_df.csv',index=False)\n",
    "    return best_hyperparameters,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c32a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_filter_outputs(model,test_loader):\n",
    "    with torch.no_grad():\n",
    "        best_model.eval()\n",
    "        pred_list = []\n",
    "        for i, (header, seq, data, target) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            pred=output.cpu().detach().numpy()\n",
    "            pred_list.append(pred)\n",
    "        predictions = np.concatenate(pred_list)\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4532222b",
   "metadata": {},
   "source": [
    "# Motif extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9ebd21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_content(pwm, transpose=False, bg_gc=0.415):\n",
    "    ''' Compute PWM information content.\n",
    "    In the original analysis, I used a bg_gc=0.5. For any\n",
    "    future analysis, I ought to switch to the true hg19\n",
    "    value of 0.415.\n",
    "    '''\n",
    "    pseudoc = 1e-9\n",
    "    if transpose:\n",
    "        pwm = np.transpose(pwm)\n",
    "\n",
    "    bg_pwm = [1-bg_gc, bg_gc, bg_gc, 1-bg_gc]\n",
    "\n",
    "    ic = 0\n",
    "    for i in range(pwm.shape[0]):\n",
    "        for j in range(4):\n",
    "            # ic += 0.5 + pwm[i][j]*np.log2(pseudoc+pwm[i][j])\n",
    "            ic += -bg_pwm[j]*np.log2(bg_pwm[j]) + pwm[i][j]*np.log2(pseudoc+pwm[i][j])\n",
    "    return ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76c17aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_intro(meme_file, seqs):\n",
    "    ''' Open MEME motif format file and print intro\n",
    "    Attrs:\n",
    "        meme_file (str) : filename\n",
    "        seqs [str] : list of strings for obtaining background freqs\n",
    "    Returns:\n",
    "        mem_out : open MEME file\n",
    "    '''\n",
    "    nts = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "\n",
    "    # count\n",
    "    nt_counts = [1]*4\n",
    "    for i in range(len(seqs)):\n",
    "        for nt in seqs[i][1]:\n",
    "            try:\n",
    "                nt_counts[nts[nt]] += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "    # normalize\n",
    "    nt_sum = float(sum(nt_counts))\n",
    "    nt_freqs = [nt_counts[i]/nt_sum for i in range(4)]\n",
    "\n",
    "    # open file for writing\n",
    "    meme_out = open(meme_file, 'w')\n",
    "\n",
    "    # print intro material\n",
    "    print( 'MEME version 4', file=meme_out)\n",
    "    print( '', file=meme_out)\n",
    "    #embd\n",
    "\n",
    "    print( 'ALPHABET= ACGT', file=meme_out)        \n",
    "    \n",
    "    print( '', file=meme_out)\n",
    "    print( 'Background letter frequencies:', file=meme_out)\n",
    "    #embd\n",
    "    print( 'A %.4f C %.4f G %.4f T %.4f' % tuple(nt_freqs), file=meme_out)\n",
    "    print( '', file=meme_out)\n",
    "    return meme_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d58a1b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter_pwm(filter_fasta):\n",
    "    ''' Make a PWM for this filter from its top hits '''\n",
    "    nts = {'A':0, 'C':1, 'G':2, 'T':3}\n",
    "    #embd\n",
    "    pwm_counts = []\n",
    "    nsites = 4 # pseudocounts\n",
    "    for line in open(filter_fasta):\n",
    "        if line[0] != '>':\n",
    "            seq = line.rstrip()\n",
    "            nsites += 1\n",
    "            if len(pwm_counts) == 0:\n",
    "                # initialize with the length\n",
    "                for i in range(len(seq)):\n",
    "                    pwm_counts.append(np.array([1.0]*4))\n",
    "\n",
    "            # count\n",
    "            for i in range(len(seq)):\n",
    "                try:\n",
    "                    pwm_counts[i][nts[seq[i]]] += 1\n",
    "                except KeyError:\n",
    "                    pwm_counts[i] += np.array([0.25]*4)\n",
    "\n",
    "    # normalize\n",
    "    pwm_freqs = []\n",
    "    for i in range(len(pwm_counts)):\n",
    "        pwm_freqs.append([pwm_counts[i][j]/float(nsites) for j in range(4)])\n",
    "\n",
    "    return np.array(pwm_freqs), nsites-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58673d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_density(f_scores, out_pdf):\n",
    "    sns.set(font_scale=1.3)\n",
    "    plt.figure()\n",
    "    sns.distplot(f_scores, kde=False)\n",
    "    plt.xlabel('ReLU output')\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()\n",
    "\n",
    "    return f_scores.mean(), f_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "948f58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_motif(param_matrix):\n",
    "    nts = 'ACGT'\n",
    "    motif_list = []\n",
    "    for v in range(param_matrix.shape[1]):\n",
    "        max_n = 0\n",
    "        for n in range(1,4):\n",
    "            if param_matrix[n,v] > param_matrix[max_n,v]:\n",
    "                max_n = n\n",
    "\n",
    "        if param_matrix[max_n,v] > 0:\n",
    "            motif_list.append(nts[max_n])\n",
    "        else:\n",
    "            motif_list.append('N')\n",
    "\n",
    "    return ''.join(motif_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc48f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_heat(param_matrix, out_pdf):\n",
    "    param_range = abs(param_matrix).max()\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(param_matrix.shape[1], 4))\n",
    "    sns.heatmap(param_matrix, cmap='PRGn', linewidths=0.2, vmin=-param_range, vmax=param_range)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(range(1,param_matrix.shape[1]+1))\n",
    "    ax.set_yticklabels('ACGT', rotation='horizontal') # , size=10)\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cebfb0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_logo(filter_outs, filter_size, seqs, out_prefix, raw_t=0, maxpct_t=None):\n",
    "    if maxpct_t:\n",
    "        all_outs = np.ravel(filter_outs)\n",
    "        all_outs_mean = all_outs.mean()\n",
    "        all_outs_norm = all_outs - all_outs_mean\n",
    "        raw_t = 0.65 * all_outs_norm.max() + all_outs_mean\n",
    "        #raw_t = 0.65 * all_outs_norm.max() + all_outs_mean\n",
    "    # print fasta file of positive outputs\n",
    "    filter_fasta_out = open('%s.fa' % out_prefix, 'w')\n",
    "    filter_count = 0\n",
    "    for i in range(filter_outs.shape[0]):\n",
    "        for j in range(filter_outs.shape[1]):\n",
    "            if filter_outs[i,j] > raw_t:\n",
    "                #print(len(seqs[i]))\n",
    "                fw.write(str(j))\n",
    "                fw.write('\\n')\n",
    "                kmer = seqs[i][1][j:j+filter_size]\n",
    "                #kmer = kmer.replace('T','U')\n",
    "                incl_kmer = len(kmer) - kmer.count('N')\n",
    "                if incl_kmer <filter_size:\n",
    "                    continue\n",
    "                print('>%d_%d' % (i,j), file=filter_fasta_out)\n",
    "                print(kmer, file=filter_fasta_out)\n",
    "                filter_count += 1\n",
    "    filter_fasta_out.close()\n",
    "    \n",
    "    # make weblogo\n",
    "    \"\"\"if filter_count > 0:\n",
    "        weblogo_cmd = 'weblogo %s < %s.fa > %s.eps' % (weblogo_opts, out_prefix, out_prefix)\n",
    "        subprocess.call(weblogo_cmd, shell=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5465afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_add(meme_out, f, filter_pwm, nsites, trim_filters=False):\n",
    "    ''' Print a filter to the growing MEME file\n",
    "    Attrs:\n",
    "        meme_out : open file\n",
    "        f (int) : filter index #\n",
    "        filter_pwm (array) : filter PWM array\n",
    "        nsites (int) : number of filter sites\n",
    "    '''\n",
    "    if not trim_filters:\n",
    "        ic_start = 0\n",
    "        ic_end = filter_pwm.shape[0]-1\n",
    "    else:\n",
    "        ic_t = 0.2\n",
    "\n",
    "        # trim PWM of uninformative prefix\n",
    "        ic_start = 0\n",
    "        while ic_start < filter_pwm.shape[0] and info_content(filter_pwm[ic_start:ic_start+1]) < ic_t:\n",
    "            ic_start += 1\n",
    "\n",
    "        # trim PWM of uninformative suffix\n",
    "        ic_end = filter_pwm.shape[0]-1\n",
    "        while ic_end >= 0 and info_content(filter_pwm[ic_end:ic_end+1]) < ic_t:\n",
    "            ic_end -= 1\n",
    "\n",
    "    if ic_start < ic_end:\n",
    "        print('MOTIF filter%d' % f, file=meme_out)\n",
    "        print('letter-probability matrix: alength= 4 w= %d nsites= %d' % (ic_end-ic_start+1, nsites), file=meme_out)\n",
    "\n",
    "        for i in range(ic_start, ic_end+1):\n",
    "            print( '%.4f %.4f %.4f %.4f' % tuple(filter_pwm[i]), file=meme_out)\n",
    "\n",
    "\n",
    "        print( '', file=meme_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d2a5d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filter_heat(param_matrix, out_pdf):\n",
    "    param_range = abs(param_matrix).max()\n",
    "\n",
    "    sns.set(font_scale=2)\n",
    "    plt.figure(figsize=(param_matrix.shape[1], 4))\n",
    "    sns.heatmap(param_matrix, cmap='PRGn', linewidths=0.2, vmin=-param_range, vmax=param_range)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(range(1,param_matrix.shape[1]+1))\n",
    "    ax.set_yticklabels('ACGT', rotation='horizontal') # , size=10)\n",
    "    plt.savefig(out_pdf)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "57d47ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_motif(filter_weights_old, filter_outs, seqs, out_dir):\n",
    "    global fw\n",
    "    filter_weights = []\n",
    "    for x in filter_weights_old:\n",
    "        x = x - np.mean(x,axis = 0)\n",
    "        filter_weights.append(x)\n",
    "        \n",
    "    filter_weights = np.array(filter_weights)\n",
    "    num_filters = filter_weights.shape[0]\n",
    "    filter_size = filter_weights.shape[2]\n",
    "    filters_ic = []\n",
    "    meme_out = meme_intro('%s/filters_meme.txt'%(out_dir), seqs)\n",
    "    fw = open('indices.txt', 'w')\n",
    "    for f in range(num_filters):\n",
    "        # plot filter parameters as a heatmap\n",
    "        plot_filter_heat(filter_weights[f,:,:filter_size], '%s/filter%d_heat.pdf' % (out_dir,f))\n",
    "\n",
    "        # plot weblogo of high scoring outputs\n",
    "        plot_filter_logo(filter_outs[:,f,:], filter_size, seqs, '%s/filter%d_logo'%(out_dir,f), maxpct_t=0.5)\n",
    "\n",
    "        # make a PWM for the filter\n",
    "        filter_pwm, nsites = make_filter_pwm('%s/filter%d_logo.fa'%(out_dir,f))\n",
    "\n",
    "        if nsites < 10:\n",
    "            # no information\n",
    "            filters_ic.append(0)\n",
    "        else:\n",
    "            # compute and save information content\n",
    "            filters_ic.append(info_content(filter_pwm))\n",
    "\n",
    "            # add to the meme motif file\n",
    "            meme_add(meme_out, f, filter_pwm, nsites, False)\n",
    "\n",
    "    meme_out.close()\n",
    "    fw.close()\n",
    "    table_out = open('%s/table.txt'%out_dir, 'w')\n",
    "\n",
    "    # print header for later panda reading\n",
    "    table = PrettyTable([\"Filter\", \"consensus\",\"annotation\",\"ic\",\"mean\",\"std\"])\n",
    "    header_cols = ('', 'consensus', 'annotation', 'ic', 'mean', 'std')\n",
    "    print('%3s  %19s  %10s  %5s  %6s  %6s' % header_cols, file=table_out)\n",
    "    \n",
    "    for f in range(num_filters):\n",
    "        # collapse to a consensus motif\n",
    "        consensus = filter_motif(filter_weights[f,:,:])\n",
    "\n",
    "        # grab annotation\n",
    "        annotation = '.'\n",
    "\n",
    "        # plot density of filter output scores\n",
    "        fmean, fstd = plot_score_density(np.ravel(filter_outs[: , f , :]), '%s/filter%d_dens.pdf' % (out_dir,f))\n",
    "\n",
    "        row_cols = (f, consensus, annotation, filters_ic[f], fmean, fstd)\n",
    "        table.add_row(list(row_cols))\n",
    "        print( '%-3d  %19s  %10s  %5.2f  %6.4f  %6.4f' % row_cols, file=table_out)\n",
    "        \n",
    "    table_out.close()\n",
    "    print(table.get_string())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d01a32e",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "618cb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "global device\n",
    "global model_dir\n",
    "global results_dir\n",
    "global data_dir\n",
    "global verbose\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_dir = 'model'\n",
    "results_dir = 'results'\n",
    "data_dir = 'Data'\n",
    "verbose = False\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c73b4a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1575, 183, 1758, 226)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = data_dir+'/S288C_reference_sequence_R64-3-1_20210421.fsa'\n",
    "peak_file = data_dir+'/Condensin_peaks_Log.bed'#data_dir+'/Condensin_peaks_quiescence.bed'\n",
    "seq_length = 500\n",
    "motif_length = 24\n",
    "cross_chrom = True\n",
    "include_dinuc = True\n",
    "calib_data,valid_data,train_data,test_data,peak_sequences_train,peak_sequences_test = extract_data(data_file,peak_file,motif_length,\n",
    "                                                                                                   seq_length,cross_chrom,include_dinuc)\n",
    "len(calib_data),len(valid_data),len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50fc972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dataset=dataset(calib_data)\n",
    "valid_dataset=dataset(valid_data)\n",
    "train_dataset=dataset(train_data)\n",
    "test_dataset=dataset(test_data)\n",
    "batch_size = 64\n",
    "calib_loader = DataLoader(dataset=calib_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "734a0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 out of 30\n",
      "early stopping at epoch  1\n",
      "model 2 out of 30\n",
      "early stopping at epoch  12\n",
      "model 3 out of 30\n",
      "early stopping at epoch  6\n",
      "model 4 out of 30\n",
      "early stopping at epoch  244\n",
      "model 5 out of 30\n",
      "early stopping at epoch  33\n",
      "model 6 out of 30\n",
      "early stopping at epoch  27\n",
      "model 7 out of 30\n",
      "early stopping at epoch  346\n",
      "model 8 out of 30\n",
      "early stopping at epoch  1\n",
      "model 9 out of 30\n",
      "early stopping at epoch  282\n",
      "model 10 out of 30\n",
      "no early stopping\n",
      "model 11 out of 30\n",
      "early stopping at epoch  12\n",
      "model 12 out of 30\n",
      "early stopping at epoch  1\n",
      "model 13 out of 30\n",
      "early stopping at epoch  290\n",
      "model 14 out of 30\n",
      "no early stopping\n",
      "model 15 out of 30\n",
      "no early stopping\n",
      "model 16 out of 30\n",
      "early stopping at epoch  218\n",
      "model 17 out of 30\n",
      "early stopping at epoch  6\n",
      "model 18 out of 30\n",
      "early stopping at epoch  43\n",
      "model 19 out of 30\n",
      "no early stopping\n",
      "model 20 out of 30\n",
      "early stopping at epoch  43\n",
      "model 21 out of 30\n",
      "early stopping at epoch  48\n",
      "model 22 out of 30\n",
      "early stopping at epoch  427\n",
      "model 23 out of 30\n",
      "early stopping at epoch  20\n",
      "model 24 out of 30\n",
      "early stopping at epoch  23\n",
      "model 25 out of 30\n",
      "no early stopping\n",
      "model 26 out of 30\n",
      "early stopping at epoch  218\n",
      "model 27 out of 30\n",
      "early stopping at epoch  8\n",
      "model 28 out of 30\n",
      "early stopping at epoch  9\n",
      "model 29 out of 30\n",
      "early stopping at epoch  19\n",
      "model 30 out of 30\n",
      "early stopping at epoch  16\n"
     ]
    }
   ],
   "source": [
    "num_motif_list = [30,40,60]\n",
    "num_conv_layers_list = [1,2]\n",
    "dropprob_list = [0, 0.15, 0.3]\n",
    "learning_rate_list = [10**-5,10**-4,10**-3,10**-2]\n",
    "max_num_models = 30\n",
    "maxepochs = 500\n",
    "epochs_for_early_stop = 50\n",
    "best_hyperparameters,results = Calibrate_model(calib_loader,valid_loader, num_motif_list, num_conv_layers_list , dropprob_list,\n",
    "                                               learning_rate_list, max_num_models=max_num_models, maxepochs=maxepochs,\n",
    "                                               epochs_for_early_stop=epochs_for_early_stop,motif_len=motif_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ab6eecc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_conv_layers</th>\n",
       "      <th>num_motif</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>epochs</th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPRC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>282</td>\n",
       "      <td>0.976293</td>\n",
       "      <td>0.977569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>218</td>\n",
       "      <td>0.963482</td>\n",
       "      <td>0.969429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>33</td>\n",
       "      <td>0.960010</td>\n",
       "      <td>0.968901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>290</td>\n",
       "      <td>0.959531</td>\n",
       "      <td>0.965703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>244</td>\n",
       "      <td>0.954502</td>\n",
       "      <td>0.961075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  num_conv_layers num_motif  Dropout  Learning Rate epochs     AUROC     AUPRC\n",
       "0               2        60     0.15         0.0001    282  0.976293  0.977569\n",
       "0               2        40     0.15         0.0001    218  0.963482  0.969429\n",
       "0               2        60     0.15         0.0010     33  0.960010  0.968901\n",
       "0               1        60     0.00         0.0001    290  0.959531  0.965703\n",
       "0               2        30     0.15         0.0001    244  0.954502  0.961075"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "322d7eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epochs': 282,\n",
       " 'best_num_motif': 60,\n",
       " 'best_num_conv_layers': 2,\n",
       " 'best_dropprob': 0.15,\n",
       " 'best_l_rate': 0.0001}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "314fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no early stopping\n",
      "+------------------------+------------+\n",
      "|        Modules         | Parameters |\n",
      "+------------------------+------------+\n",
      "|  conv_layer.0.weight   |    5760    |\n",
      "|   conv_layer.0.bias    |     60     |\n",
      "|  conv_layer.3.weight   |   64800    |\n",
      "|   conv_layer.3.bias    |     90     |\n",
      "| project.to_attn_logits |    8100    |\n",
      "|  classifier.0.weight   |    8100    |\n",
      "|   classifier.0.bias    |     90     |\n",
      "|  classifier.3.weight   |     90     |\n",
      "|   classifier.3.bias    |     1      |\n",
      "+------------------------+------------+\n",
      "\n",
      " Total Trainable Params: 87091\n"
     ]
    }
   ],
   "source": [
    "maxepochs = best_hyperparameters['best_epochs']\n",
    "num_motif = best_hyperparameters['best_num_motif']\n",
    "num_conv_layers = best_hyperparameters['best_num_conv_layers']\n",
    "dropprob = best_hyperparameters['best_dropprob']\n",
    "l_rate = best_hyperparameters['best_l_rate']\n",
    "epochs_for_early_stop = 0\n",
    "model = Network(num_motif , motif_length , num_conv_layers , dropprob).to(device)\n",
    "best_model,epochs,train_losses,valid_losses = Train_model(model,train_loader,valid_loader,l_rate ,maxepochs,\n",
    "                                                          epochs_for_early_stop,save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fda8ee23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9476074868822931, 0.9574728745148315)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc,prc = Test_model(best_model,test_loader)\n",
    "auc,prc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "227ac032",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = best_model.conv[0].weight.detach().cpu().numpy()\n",
    "bias = best_model.conv[0].bias.detach().cpu().numpy()\n",
    "motif_sequences=generate_onehot_data(peak_sequences_test,motif_length,1,include_dinuc=False)\n",
    "motif_dataset=dataset(motif_sequences)\n",
    "motif_loader = DataLoader(dataset=motif_dataset,\n",
    "                          batch_size=batch_size,shuffle=False)\n",
    "out_model = conv_output(weights,bias,device)\n",
    "filter_output = return_filter_outputs(out_model,motif_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b81cbeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 4, 24)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3342abf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 60, 523)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0f0385d",
   "metadata": {},
   "source": [
    "# Motif extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17059f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------+------------+-----------------------+-------------+-------------+\n",
      "| Filter |        consensus         | annotation |           ic          |     mean    |     std     |\n",
      "+--------+--------------------------+------------+-----------------------+-------------+-------------+\n",
      "|   0    | AAAACAATTAGACTGAAAAAGCTG |     .      |  0.17864202909730897  |  0.20100093 |  0.2292801  |\n",
      "|   1    | AGCTGCTGCCTAAATGTATCATGC |     .      |   1.0209365066142762  |  0.24851735 |  0.23527859 |\n",
      "|   2    | CGCCGCGAGAAATCATTAATCGCT |     .      |   1.0348282778471212  |  0.1477161  |  0.1841126  |\n",
      "|   3    | GCGACGGGGTGCGACGAGCCCCCC |     .      |  0.03037135849468614  |  0.14012165 |  0.16896287 |\n",
      "|   4    | AAGTACGGCCGACGCACTGAATCG |     .      |   0.5945036300333117  | 0.100685075 |  0.14922039 |\n",
      "|   5    | TAATGGACAATAGGGTACGGCTAG |     .      |  0.23678172330564856  |   0.118984  |  0.18848166 |\n",
      "|   6    | TAAAGCCTTACGGCTTATTGAGAC |     .      |   0.506387683376536   |  0.11364916 |  0.1606802  |\n",
      "|   7    | TGACCGTCTGCCTGTCTTGATTTT |     .      |   0.9704716938229607  |  0.34362236 |  0.3202056  |\n",
      "|   8    | CGAGTTTCCCCTGGGGGGGGGCTG |     .      |   0.0888260315865142  |  0.18079062 |  0.21013393 |\n",
      "|   9    | GCCTAGTCAGACGGCAAATCGACT |     .      |  0.35991938377978433  |  0.1150306  |  0.17071201 |\n",
      "|   10   | GCGGTTCCGCCGGCGTCTTTCCCG |     .      |   0.5072568157756006  |  0.12326688 |  0.17263897 |\n",
      "|   11   | ACTCAGTGTTGCTTCTACCGCGGT |     .      |   0.6650096610178822  |  0.12530455 |  0.2006386  |\n",
      "|   12   | TTTATTTTCATATTATGATATACG |     .      |   0.9202180048435713  |  0.28244513 |  0.3178614  |\n",
      "|   13   | TTAAACCGCCCTAAGCATCAGCCA |     .      |   0.4922249076184693  |  0.1912913  |  0.20478569 |\n",
      "|   14   | AACCGCGGTTTGACCTGACCCGCG |     .      |   1.4673670282605376  |  0.09220482 |  0.14545844 |\n",
      "|   15   | CCGAACTATAGTGAATCATGAGCT |     .      |   0.8787897205998838  | 0.060665518 | 0.120893545 |\n",
      "|   16   | ACTTCTCTATGTCTATTATTGCTG |     .      |   1.334977420655224   |  0.37727332 |  0.33020443 |\n",
      "|   17   | CACGCTATAATAAGGAGGCTTCGT |     .      |   0.8684222921150915  |  0.06832097 |  0.12634715 |\n",
      "|   18   | ATTGTGAAGGCATGTGCATAGATG |     .      |   1.030052285274175   |  0.15014897 |  0.22924906 |\n",
      "|   19   | CCGGGGTAGCGAACGCTGAAACCA |     .      | -0.020092789530501143 |  0.11665739 |  0.17422469 |\n",
      "|   20   | CTGGCGGCTTGCCTAATAAATTGT |     .      |  0.49599216482056246  |  0.20837615 |  0.2239885  |\n",
      "|   21   | TGATAGTGTCGTTGTCGGTTTTTT |     .      |   0.7520828900123802  |  0.12680791 |  0.18694249 |\n",
      "|   22   | GCGGGGTGACGAACTCGAGTCTTT |     .      |  0.12913355410197558  |  0.18962255 |  0.24417035 |\n",
      "|   23   | TTTTGACCTTGATGACGCTATTGG |     .      |   1.1342465274136135  |  0.14227472 |  0.1870908  |\n",
      "|   24   | GCTGTGGCCGGGGATTGAGTGGCT |     .      |  0.33008102257263416  |  0.29704157 |  0.25960937 |\n",
      "|   25   | GTGAAAATTTCGAAATACCGTCAC |     .      |   0.9126866731407075  |  0.09219343 |  0.15359655 |\n",
      "|   26   | GCGCACTAAGCACTTATAAGCCCC |     .      |   0.4637659761945631  |  0.13554133 |  0.17332582 |\n",
      "|   27   | TATTCATTGAGTTTTACTAACTGA |     .      |   1.3210035967165723  |  0.18206833 |  0.20756158 |\n",
      "|   28   | AGATACATGGGGAGCCGAAAGACG |     .      |   0.4288157038978861  |  0.1305684  |  0.19209537 |\n",
      "|   29   | CGGATCTTATTTAAATCTTATTAC |     .      |    1.4224579123641    |  0.11808429 |  0.18431272 |\n",
      "|   30   | CTATTGACGAAGCGGCAACAAAAG |     .      |   0.9712832608388902  |  0.20895489 |  0.23653428 |\n",
      "|   31   | AGAAAGAAAGACCGAACAAGCACA |     .      |   1.0087014775285479  |  0.18143147 |  0.2156619  |\n",
      "|   32   | AATGGTACACCTGACGGCCAAGGC |     .      |   0.7117693226881001  |  0.13550033 |  0.21195462 |\n",
      "|   33   | GCCCGCGTTGTTTAATGTCAGACG |     .      |   0.5717909033695783  |  0.24886419 |  0.23834217 |\n",
      "|   34   | TATGACTACGAATACTCTGGTTAC |     .      |   0.8958061304845577  |  0.07952243 |  0.13810709 |\n",
      "|   35   | TACAGCAGCGGCCTTCGCAGCCGC |     .      |   0.9306891171376097  |  0.11586376 |  0.16528802 |\n",
      "|   36   | TATGTTATATACCTGTTCGTATTT |     .      |    1.34094263910888   |  0.13293615 |  0.22130966 |\n",
      "|   37   | GGTTCGTTCCGTCTCCTCCGTCTG |     .      |  0.27976789819999665  |  0.24454984 |  0.23119886 |\n",
      "|   38   | CGGAACCTAAACCTAGATTCGCTG |     .      |   0.5857976510395018  |  0.2776618  |  0.27483317 |\n",
      "|   39   | ACGCCTCAGGCGGGTAATTACTAA |     .      |   0.4830298581305594  |  0.21702297 |  0.2119039  |\n",
      "|   40   | CACCGGCAGGGGCGCTGCCGGCGC |     .      |   0.1255805631275025  |  0.13111414 |  0.18358316 |\n",
      "|   41   | CGTTTTTAAACAGCAATACATACT |     .      |   1.409172009424356   |  0.19493096 |  0.22090058 |\n",
      "|   42   | GCCTACTAAGACCGACGCTCTTTA |     .      |   0.679325907192541   |  0.18909836 |  0.20524706 |\n",
      "|   43   | GTACCGCGCGCACTGCTCACCCAC |     .      |  -0.02977480693342427 |  0.19336584 |  0.21487369 |\n",
      "|   44   | ATATAGACTCAAGAGATATTAGAT |     .      |   0.7531066257028356  |  0.16907683 |  0.21370786 |\n",
      "|   45   | TCCGCCGCGCTAATTAGCTACCAC |     .      |   0.0813723153410662  | 0.100269005 |  0.15349568 |\n",
      "|   46   | GATAGTTACGTACGAGTGTCTCGC |     .      |   0.2565789830777799  |  0.17571042 |  0.19933037 |\n",
      "|   47   | ACCTTAAGACTGGCATCCTTTGCC |     .      |  0.09703761311641429  |  0.12082284 |  0.1963616  |\n",
      "|   48   | TGATGCTTAATAACTCATAAATAC |     .      |    1.45877846278278   | 0.109779626 |  0.17809531 |\n",
      "|   49   | GAGAAGAGCTGAGCGGCGTAAAAA |     .      |   0.694337015222342   |  0.14275505 |  0.17211652 |\n",
      "|   50   | AACGTACCCTGCCCCTGGTGGCTG |     .      |   0.7069359996380602  |  0.14618342 |  0.17434336 |\n",
      "|   51   | CCCCGCCTACGTTCTTATTACTTT |     .      |   0.5700836738541619  |  0.20924662 |  0.22201389 |\n",
      "|   52   | CGCAATGGCCGCTCCACCCCACGA |     .      |   0.4218097572951127  |  0.24518922 |  0.2931035  |\n",
      "|   53   | GTCTTCGTCCGTCGCACCGAATCG |     .      |  0.46345222826260496  |  0.11504874 |  0.16335534 |\n",
      "|   54   | CGCCGAGTGCCTATATTAGTAACA |     .      |   1.7033793484274808  | 0.124766424 |  0.18513945 |\n",
      "|   55   | CCTTTCGTCGCCCTGGTTCGGCCT |     .      |  0.07533856772484071  |  0.1527983  |   0.183931  |\n",
      "|   56   | CAACAACGAAAGGGGCAGAAGCAA |     .      |   0.3425946652737874  |  0.09938275 |  0.16079657 |\n",
      "|   57   | GATTTCCTTGACGGTACTATCAAT |     .      |   0.8754305296654614  | 0.077636525 |  0.14021707 |\n",
      "|   58   | GACGGGATCATCGGGTCCATCGTC |     .      |  0.026313270820200407 |  0.13791905 |  0.17766203 |\n",
      "|   59   | CTTCTAGGCTATGATATTTTATCT |     .      |   1.0806551453813098  |  0.08783541 |  0.16311435 |\n",
      "+--------+--------------------------+------------+-----------------------+-------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "get_motif(weights,filter_output,peak_sequences_test,results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e704dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
